{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde84685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torchvision.ops as ops\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bce77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### General Parameters ####\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "PATCH_SIZE = 8\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "BERT_FREEZE_LAYERS = 8\n",
    "MODEL_DIM = 768\n",
    "LR_BERT = 2e-5\n",
    "LR_VIT = 2e-4\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b08dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "        \n",
    "        for name, param in self.bert.named_parameters():\n",
    "            if 'encoder.layer' in name:\n",
    "                layer_num = int(name.split('encoder.layer.')[1].split('.')[0])\n",
    "                if layer_num < BERT_FREEZE_LAYERS:\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "        encoded_text = self.tokenizer(texts, padding=True, truncation=True, \n",
    "                                      return_tensors=\"pt\", max_length=128).to(device=DEVICE)\n",
    "\n",
    "        return encoded_text\n",
    "    \n",
    "    def forward(self, encoded_text):\n",
    "        outputs = self.bert(**encoded_text)\n",
    "        \n",
    "        return outputs.last_hidden_state.to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d369743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_channels=1,\n",
    "        embed_dim=MODEL_DIM,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        num_heads=12,\n",
    "    ):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = [embed_dim // 8, embed_dim // 4, embed_dim // 2, embed_dim]\n",
    "\n",
    "        self.patch_embed = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                embed_dim // 8,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        self.position_embedding = nn.ParameterList(\n",
    "            [\n",
    "                nn.Parameter(torch.zeros(1, NUM_PATCHES, d), requires_grad=True)\n",
    "                for d in self.channels\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        dim = embed_dim // 8\n",
    "        for i, depth in enumerate(depths):\n",
    "            self.encoders.append(\n",
    "                nn.TransformerEncoder(\n",
    "                    nn.TransformerEncoderLayer(\n",
    "                        d_model=dim,\n",
    "                        nhead=num_heads // (2 ** (3 - i)),\n",
    "                        dim_feedforward=int(dim * 3.5),\n",
    "                        batch_first=True,\n",
    "                    ),\n",
    "                    num_layers=depth,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.upsamplers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(dim), nn.Linear(in_features=dim, out_features=dim * 2)\n",
    "                )\n",
    "                if i < len(depths) - 1\n",
    "                else nn.Identity()\n",
    "            )\n",
    "\n",
    "            dim *= 2\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patch_embed(X)  # [B, 96, 32, 32]\n",
    "\n",
    "        features = []\n",
    "        for i, (encoder, upsampler, position) in enumerate(\n",
    "            zip(self.encoders, self.upsamplers, self.position_embedding)\n",
    "        ):\n",
    "            X_seq_src = rearrange(X, \"b c h w -> b (h w) c\")\n",
    "            X_seq_new = encoder(X_seq_src + position)\n",
    "            features.append(X_seq_new)\n",
    "            \n",
    "            if i < len(self.upsamplers) - 1:\n",
    "                X = upsampler(X_seq_new + X_seq_src)  # [B, 1024, C*2]\n",
    "                X = rearrange(\n",
    "                    X, \"b (h w) c -> b c h w\", h=self.img_size // self.patch_size\n",
    "                )\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f519ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMAGE_SIZE,\n",
    "        embed_dim=MODEL_DIM,\n",
    "        num_heads=12,\n",
    "        dropout=0.1,\n",
    "        mlp_ratio=3.5,\n",
    "        output_hidden=2048,\n",
    "        output_dim=1024,\n",
    "    ):\n",
    "        super(VisionDecoder, self).__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.dropout = dropout\n",
    "        self.channels = [embed_dim // 8, embed_dim // 4, embed_dim // 2, embed_dim]\n",
    "\n",
    "        self.cross_attention = nn.ModuleList()\n",
    "        self.pre_mlp_norm = nn.ModuleList()\n",
    "        self.post_mlp_norm = nn.ModuleList()\n",
    "        self.mlp_blocks = nn.ModuleList()\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "\n",
    "        for i, dim in enumerate(self.channels):\n",
    "            self.cross_attention.append(\n",
    "                nn.MultiheadAttention(\n",
    "                    embed_dim=dim,\n",
    "                    num_heads=num_heads // (2 ** (3 - i)),\n",
    "                    dropout=dropout,\n",
    "                    kdim=embed_dim,\n",
    "                    vdim=embed_dim,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.pre_mlp_norm.append(nn.LayerNorm(dim))\n",
    "            self.post_mlp_norm.append(nn.LayerNorm(dim))\n",
    "            self.mlp_blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_features=dim, out_features=int(mlp_ratio * dim)),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(in_features=int(mlp_ratio * dim), out_features=dim)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.upsamplers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_features=dim, out_features=dim * 2), nn.LayerNorm(dim * 2)\n",
    "                )\n",
    "                if i < len(self.channels) - 1\n",
    "                else nn.LayerNorm(dim)\n",
    "            )\n",
    "\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            *[\n",
    "                nn.Linear(in_features=embed_dim, out_features=output_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(in_features=output_hidden, out_features=output_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(in_features=output_hidden, out_features=output_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Linear(in_features=output_hidden, out_features=output_dim),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, img_features, T):\n",
    "        features = []\n",
    "        for i, (attn, pre_norm, post_norm, mlp, upsampler) in enumerate(\n",
    "            zip(\n",
    "                self.cross_attention,\n",
    "                self.pre_mlp_norm,\n",
    "                self.post_mlp_norm,\n",
    "                self.mlp_blocks,\n",
    "                self.upsamplers,\n",
    "            )\n",
    "        ):\n",
    "            src = img_features[i]\n",
    "            if i > 0:\n",
    "                src += features[i - 1]\n",
    "\n",
    "            X, _ = attn(query=src, key=T, value=T, need_weights=False)\n",
    "        \n",
    "            X = F.dropout(X, p=self.dropout, training=self.training)\n",
    "            X = pre_norm(X + src)\n",
    "\n",
    "            Xff = mlp(X)\n",
    "\n",
    "            Xff = F.dropout(Xff, p=self.dropout, training=self.training)\n",
    "            X = post_norm(X + Xff)\n",
    "\n",
    "            X = upsampler(X)\n",
    "            features.append(X)\n",
    "\n",
    "        return self.final_mlp(features[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f36933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionOutputHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim=1024,\n",
    "    ):\n",
    "        super(VisionOutputHead, self).__init__()\n",
    "\n",
    "        self.binary_output = nn.Sequential(\n",
    "            *[\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_features=in_dim, out_features=in_dim // 4),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(in_features=in_dim // 4, out_features=1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.count_output = nn.Sequential(\n",
    "            *[\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_features=in_dim, out_features=in_dim // 4),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(in_features=in_dim // 4, out_features=in_dim // 16),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(in_features=in_dim // 16, out_features=1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        binary = self.binary_output(X)\n",
    "        count = self.count_output(X)\n",
    "\n",
    "        return binary, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e081758",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_channels=1,\n",
    "        embed_dim=MODEL_DIM,\n",
    "        encoder_depths=[2, 2, 2, 2],\n",
    "        output_dim=1024,\n",
    "    ):\n",
    "        super(MultiModalViT, self).__init__()\n",
    "        \n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "        self.encoder = VisionEncoder(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim,\n",
    "            depths=encoder_depths,\n",
    "        )\n",
    "\n",
    "        self.decoder = VisionDecoder(\n",
    "            img_size=img_size, embed_dim=embed_dim, output_dim=output_dim\n",
    "        )\n",
    "\n",
    "        self.output = VisionOutputHead(in_dim=output_dim)\n",
    "    \n",
    "    def forward(self, imgs, texts):\n",
    "        text_embeddings = self.text_encoder.tokenize(texts)\n",
    "        text_embeddings = self.text_encoder(text_embeddings)\n",
    "        \n",
    "        features = self.encoder(imgs)\n",
    "        \n",
    "        decoded_out = self.decoder(features, text_embeddings)\n",
    "        binary, count = self.output(decoded_out)\n",
    "        \n",
    "        return binary.squeeze(-1), count.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from data import (\n",
    "    Datapoint,\n",
    "    SegmentationDataset,\n",
    "    generate_labels,\n",
    "    normalize_images,\n",
    "    enhance_contrast,\n",
    "    enhance_edges,\n",
    ")\n",
    "from typing import List\n",
    "\n",
    "\n",
    "label_map = {}\n",
    "with open(\"data/combined_label_index.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        label_map[int(row[\"id\"]) - 1] = row[\"name\"].replace(\"_\", \" \")\n",
    "\n",
    "matches = glob.glob('data/**/*.npy', recursive=True)\n",
    "data: List[Datapoint] = []\n",
    "view_map = {\n",
    "    \"AP\": \"anteroposterior\",\n",
    "    \"PA\": \"posteroanterior\",\n",
    "    \"LLAT\": \"left lateral\",\n",
    "    \"RLAT\": \"right lateral\",\n",
    "}\n",
    "\n",
    "mask_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(\n",
    "            size=(256, 256), interpolation=v2.InterpolationMode.BICUBIC, antialias=True\n",
    "        ),\n",
    "        v2.ToDtype(dtype=torch.float32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms = v2.Compose(\n",
    "    [\n",
    "        v2.Lambda(lambda x: (rearrange(x, \"c h w -> h w c\") * 255).astype(np.uint8)),\n",
    "        v2.Lambda(enhance_contrast),\n",
    "        v2.Lambda(enhance_edges),\n",
    "        v2.Lambda(\n",
    "            lambda x: (\n",
    "                rearrange(x, \"h w c -> c h w\", h=IMAGE_SIZE, w=IMAGE_SIZE) / 255\n",
    "            ).astype(np.float32)\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, file_path in enumerate(matches):\n",
    "    print(file_path)\n",
    "    \n",
    "    images = rearrange(np.load(file_path), \"c b h w -> b c h w\")\n",
    "    images = mask_transforms.forward(torch.tensor(images)).numpy()\n",
    "    images = normalize_images(images)\n",
    "\n",
    "    base = np.sum(images, axis=0)\n",
    "    base = normalize_images(base)\n",
    "    base = transforms.forward(base)\n",
    "    images = images[1:, ...]\n",
    "\n",
    "    view = view_map[os.path.basename(file_path).rstrip(\"_labels.npy\")]\n",
    "    for j in range(images.shape[0]):\n",
    "        structure = label_map[j]\n",
    "        mask: np.ndarray = images[j]\n",
    "        \n",
    "        p1 = np.percentile(mask.flatten(), [5])\n",
    "        if np.count_nonzero(mask > p1) == 0 and random.random() < 0.8:\n",
    "            continue # reduce number of empty examples\n",
    "            \n",
    "        binary, count = generate_labels(mask=(mask > p1), patch_size=PATCH_SIZE)\n",
    "        data.append(\n",
    "            Datapoint(\n",
    "                base_img=base,\n",
    "                gt_mask=mask,\n",
    "                label_binary=binary,\n",
    "                label_count=count,\n",
    "                structure=structure,\n",
    "                view=view,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2924110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = SegmentationDataset(data)\n",
    "prompt, base, binary, count, mask = dataset[200]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 16))\n",
    "axs[0].imshow(rearrange(mask.numpy(), 'c h w -> h w c'), cmap='gray')\n",
    "axs[1].imshow(binary.numpy(), cmap='binary', vmin=0, vmax=1)\n",
    "axs[2].imshow(count.numpy())\n",
    "axs[3].imshow(rearrange(base.numpy(), 'c h w -> h w c'), cmap='gray')\n",
    "\n",
    "print(dataset.sentence_from_tensor(prompt))\n",
    "\n",
    "empty_examples = 0\n",
    "for (prompt, base, binary, count, mask) in dataset:\n",
    "    if np.count_nonzero(binary.numpy()) == 0:\n",
    "        empty_examples += 1\n",
    "\n",
    "print(f'Empty Examples: {empty_examples}; Nonempty Examples: {len(dataset) - empty_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, warmup_steps):\n",
    "    if step < warmup_steps:\n",
    "        return step / warmup_steps\n",
    "\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def training(model, dataset: SegmentationDataset, epochs=20, warmup_steps=100):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    text_encoder_params = {\n",
    "        \"params\": [p for p in model.text_encoder.parameters() if p.requires_grad]\n",
    "    }\n",
    "    other_params = {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for p in model.parameters()\n",
    "            if p.requires_grad\n",
    "            and not any(param is p for param in model.text_encoder.parameters())\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [0.95, 0.05])\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        [\n",
    "            {**text_encoder_params, \"lr\": LR_BERT},\n",
    "            {**other_params, \"lr\": LR_VIT},\n",
    "        ],\n",
    "        weight_decay=1e-5,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: get_lr(step, warmup_steps),\n",
    "    )\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for j, (prompt, base, binary_label, count_label, _) in enumerate(train_loader):\n",
    "            texts = []\n",
    "            for z in range(prompt.shape[0]):\n",
    "                texts.append(dataset.sentence_from_tensor(prompt[z]))\n",
    "\n",
    "            base = base.to(DEVICE, dtype=torch.float32)\n",
    "            binary_label = binary_label.flatten(start_dim=1).to(\n",
    "                DEVICE, dtype=torch.float32\n",
    "            )\n",
    "            count_label = count_label.flatten(start_dim=1).to(\n",
    "                DEVICE, dtype=torch.float32\n",
    "            )\n",
    "\n",
    "            #############################\n",
    "\n",
    "            binary, count = model(base, texts)\n",
    "            binary_prob = torch.sigmoid(binary)\n",
    "\n",
    "            bce_loss = ops.focal_loss.sigmoid_focal_loss(\n",
    "                binary, binary_label, gamma=2.5, alpha=0.75, reduction=\"mean\"\n",
    "            )\n",
    "            mse_loss = F.mse_loss(count, count_label)\n",
    "\n",
    "            confidence = torch.max(binary_prob, 1 - binary_prob)\n",
    "            diff = torch.abs(binary_prob - (count / PATCH_SIZE**2))\n",
    "            consistency_loss = (diff * confidence).mean()\n",
    "\n",
    "            loss = bce_loss + 0.6 * mse_loss + consistency_loss\n",
    "            train_loss += loss.item()\n",
    "            if j % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {i}; Batch {j} : bce={bce_loss.item():.4f}, mse={mse_loss.item():.4f}, consistency={consistency_loss.item():.4f} total={loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for j, (prompt, base, binary_label, count_label, _) in enumerate(\n",
    "                val_loader\n",
    "            ):\n",
    "                texts = []\n",
    "                for z in range(prompt.shape[0]):\n",
    "                    texts.append(dataset.sentence_from_tensor(prompt[z]))\n",
    "\n",
    "                base = base.to(DEVICE, dtype=torch.float32)\n",
    "                binary_label = binary_label.flatten(start_dim=1).to(\n",
    "                    DEVICE, dtype=torch.float32\n",
    "                )\n",
    "                count_label = count_label.flatten(start_dim=1).to(\n",
    "                    DEVICE, dtype=torch.float32\n",
    "                )\n",
    "\n",
    "                #############################\n",
    "\n",
    "                binary, count = model(base, texts)\n",
    "                binary_prob = torch.sigmoid(binary)\n",
    "\n",
    "                bce_loss = ops.focal_loss.sigmoid_focal_loss(\n",
    "                    binary, binary_label, gamma=2.5, alpha=0.75, reduction=\"mean\"\n",
    "                )\n",
    "                mse_loss = F.mse_loss(count, count_label)\n",
    "\n",
    "                confidence = torch.max(binary_prob, 1 - binary_prob)\n",
    "                diff = torch.abs(binary_prob - (count / PATCH_SIZE**2))\n",
    "                consistency_loss = (diff * confidence).mean()\n",
    "\n",
    "                loss = bce_loss + 0.6 * mse_loss + consistency_loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {i}; : training loss={train_loss:.4f}, validation loss={val_loss:.4f}\"\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if (i+1) % 2 == 0:\n",
    "            torch.save(model.state_dict(), f'checkpoints/vit_v0_epoch{32+i+1}.pt')\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c876d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalViT(\n",
    "    img_size=IMAGE_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=1,\n",
    "    embed_dim=MODEL_DIM,\n",
    "    encoder_depths=[2, 2, 4, 6],\n",
    "    output_dim=1024\n",
    ")\n",
    "model.load_state_dict(torch.load('checkpoints/vit_v0_epoch36.pt', weights_only=True))\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(f\"Total Model Parameters: ~{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}mil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = training(\n",
    "    model=model, dataset=dataset, epochs=18, warmup_steps=0\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].plot(train_losses)\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Training Loss\")\n",
    "\n",
    "axs[0].plot(val_losses)\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Validation Loss\")\n",
    "\n",
    "fig.show(warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        (prompt, base, binary_label, count_label, mask) = dataset[\n",
    "            random.randrange(0, len(dataset))\n",
    "        ]\n",
    "\n",
    "        texts = [dataset.sentence_from_tensor(prompt)]\n",
    "        # texts = [\"hip in a left lateral view xray\"]\n",
    "\n",
    "        base = base.unsqueeze(0).to(DEVICE, dtype=torch.float32)\n",
    "        binary_label = binary_label.unsqueeze(0).flatten(start_dim=1).to(\n",
    "            DEVICE, dtype=torch.float32\n",
    "        )\n",
    "        count_label = count_label.unsqueeze(0).flatten(start_dim=1).to(\n",
    "            DEVICE, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        #############################\n",
    "\n",
    "        binary, count = model(base, texts)\n",
    "        binary_prob = torch.sigmoid(binary)\n",
    "\n",
    "        bce_loss = ops.focal_loss.sigmoid_focal_loss(\n",
    "            binary, binary_label, gamma=2.5, alpha=0.75, reduction=\"mean\"\n",
    "        )\n",
    "        mse_loss = F.mse_loss(count, count_label)\n",
    "\n",
    "        confidence = torch.max(binary_prob, 1 - binary_prob)\n",
    "        diff = torch.abs(binary_prob - (count / PATCH_SIZE**2))\n",
    "        consistency_loss = (diff * confidence).mean()\n",
    "\n",
    "        loss = bce_loss + 0.6 * mse_loss + consistency_loss\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        patches = IMAGE_SIZE // PATCH_SIZE\n",
    "        binary = F.sigmoid(binary).reshape(patches, patches)\n",
    "        count = count.reshape(patches, patches)\n",
    "        binary_label = binary_label.reshape(patches, patches)\n",
    "        count_label = count_label.reshape(patches, patches)\n",
    "    \n",
    "    if loss.item() > 0.0:\n",
    "        break\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(4, 8))\n",
    "axs[0, 0].imshow(rearrange(base[0].cpu().detach().numpy(), \"c h w -> h w c\"), cmap=\"gray\")\n",
    "axs[0, 1].imshow(rearrange(mask.cpu().detach().numpy(), \"c h w -> h w c\"), cmap=\"gray\")\n",
    "axs[1, 0].imshow(\n",
    "    binary_label.cpu().detach().numpy(),\n",
    "    cmap=\"binary\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "axs[1, 1].imshow(count_label.cpu().detach().numpy())\n",
    "axs[2, 0].imshow(\n",
    "    binary.cpu().detach().numpy(),\n",
    "    cmap=\"binary\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    ")\n",
    "axs[2, 1].imshow(count.cpu().detach().numpy())\n",
    "\n",
    "print(f\"Prompt: {texts[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
